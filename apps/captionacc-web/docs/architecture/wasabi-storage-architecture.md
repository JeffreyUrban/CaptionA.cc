# Wasabi Object Storage Architecture

**Status**: Planning
**Last Updated**: 2026-01-05
**Owner**: Architecture Team

---

## Executive Summary

Migrate frame storage from SQLite BLOBs to Wasabi object storage with WebM VP9 encoded chunks to:

- **Reduce bandwidth costs**: Direct browser downloads from Wasabi (free egress) vs fly.io (paid egress)
- **Reduce storage size**: 205MB SQLite → ~75MB VP9 chunks (64% reduction)
- **Enable multi-tenancy**: Clean user separation for future Supabase auth integration
- **Improve performance**: Hardware-accelerated VP9 decode + adaptive loading based on annotation length

**Key Design Decisions**:

- **Modulo levels**: [16, 4, 1] (3 levels, powers of 4, optimized for scrolling and jumps)
- **Non-duplicating strategy**: Each frame stored in exactly ONE modulo level (no duplication, ~30MB per video)
- **Three-tier loading priorities**: Jump loading (highest), normal progressive (medium), next annotation preload (lowest)
- **Smart cache pinning**: Active + next annotation frames protected from eviction (±20 frame buffer)
- **Storage per video**: ~30MB (vs 205MB SQLite, 85% reduction)

**Based on real annotation data**: 28,449 confirmed annotations analyzed (median 1.4 sec, P95 3.7 sec)

---

## Architecture Overview

### 1. Bucket Strategy

**Single bucket with environment prefixes:**

```
captionacc-media/
├── prod/users/{userId}/videos/{videoId}/...
├── dev/users/{userId}/videos/{videoId}/...
└── ci/test-{runId}/users/{userId}/videos/{videoId}/...
```

**Rationale**:

- User-first hierarchy enables clean deletion of all user data
- Supports all user content (not just frames): videos, frames, databases, etc.
- Environment isolation via top-level prefixes
- Simpler management: single credential set, one bucket to monitor

### 2. Path Structure

**Template**: `{env}/users/{userId}/videos/{videoId}/{contentType}/{filename}`

**Content Types**:

- `original/` - Original uploaded video file
- `full_frames/` - Full frames (0.1 Hz sampling, ~6/min)
- `cropped_frames/` - Cropped frames (10 Hz sampling, 600/min)
- `median_frames/` - Median frames calculated from ranges
- `database/` - SQLite databases (optional, for future migration)

**Examples**:

```
prod/users/usr_a4f2b8c3/videos/vid_228cd4c9/original/video.mp4
prod/users/usr_a4f2b8c3/videos/vid_228cd4c9/full_frames/frame_0000000000.jpg
prod/users/usr_a4f2b8c3/videos/vid_228cd4c9/cropped_frames/frame_0000000100.jpg
prod/users/usr_a4f2b8c3/videos/vid_228cd4c9/median_frames/annotation_42.jpg
```

**Frame Naming Convention**:

- Zero-padded 10-digit index: `frame_0000000100.jpg` (frame index 100)
- Enables lexicographic sorting
- Matches existing 10 Hz indexing: frame_index = timestamp_seconds × 10

**Design Benefits**:

- ✅ Clean user deletion: Delete `{env}/users/{userId}/` prefix
- ✅ Clean video deletion: Delete `{env}/users/{userId}/videos/{videoId}/` prefix
- ✅ Clear namespace separation by content type
- ✅ Supports future content types (thumbnails, audio tracks, etc.)
- ✅ Prefixed IDs (`usr_`, `vid_`) improve debugging

### 3. Security Model

#### Phase 1: Pre-Authentication (Weeks 1-4)

**Before Supabase auth is implemented:**

- Use placeholder userId: `default_user`
- All paths: `{env}/users/default_user/videos/{videoId}/...`
- No authorization checks (single-user mode)
- Backend has full read/write access
- No browser upload permissions

#### Phase 2: Multi-User with Supabase Auth (Week 5+)

**After Supabase auth is implemented:**

**Backend Access** (processing workflows only):

- Long-lived API keys stored in environment variables
- Full read/write access for frame extraction, video processing
- Only backend code can upload to Wasabi

**TODO**: Implement separate IAM policies:

- User-facing signed URLs: GetObject only (download-only)
- Backend service credentials: Full permissions (PutObject, DeleteObject, etc.)
- Currently using single credential set with full permissions

**User Access** (browser, download-only):

- Time-limited signed URLs (1 hour expiry)
- Read-only permissions (GetObject only)
- Generated by backend after authorization check
- Users cannot upload directly to Wasabi

**Authorization Flow**:

```
Browser → Backend API
  ↓
Backend verifies Supabase session
  ↓
Backend checks: session.user.id === video.owner_id
  ↓
If authorized: Generate signed URL with GetObject permission
  ↓
Return signed URL to browser
  ↓
Browser downloads directly from Wasabi (free egress)
```

**Key Security Principle**: **Backend mediates all uploads. Users only download.**

- All videos processed by backend before storage
- Users cannot inject arbitrary content
- Rate limiting applied per user
- Quota enforcement based on subscription tier

---

## Data Storage Design

### WebM VP9 Encoded Chunks - Non-Duplicating Strategy

**Based on real annotation data** (28,449 confirmed annotations analyzed):

- **Median**: 14 frames (1.4 sec)
- **P95**: 37 frames (3.7 sec)
- **P99**: 165 frames (16.5 sec)
- **Longest**: 3021 frames (302 sec)

### Modulo Levels: [16, 4, 1]

**3 levels (powers of 4), non-duplicating - each frame stored exactly once**:

**Modulo 16** (every 1.6 sec):

- ~1,875 frames total (frames where index % 16 === 0)
- Coarse overview for 50-minute videos
- Captures annotation boundaries well
- For ±200 frame scroll: ~25 frames (good coverage)

**Modulo 4** (every 0.4 sec):

- ~5,625 frames (frames where index % 4 === 0 AND index % 16 !== 0)
- Fine detail for scrolling within annotations
- Smooth scrubbing
- **Excludes multiples of 16** (no duplication)

**Modulo 1** (remaining frames):

- ~22,500 frames (frames where index % 4 !== 0)
- **Fills the gaps** between modulo_4 and modulo_16 frames
- **No duplication** - only contains frames not in higher modulos

### Non-Duplicating Frame Assignment

**Each frame stored in exactly one modulo level**:

```python
# Frame assignment - mutually exclusive
if frame_index % 16 == 0:
    store_in("modulo_16")      # 1,875 frames
elif frame_index % 4 == 0:
    store_in("modulo_4")       # 5,625 frames (excludes mod 16)
else:  # frame_index % 4 != 0
    store_in("modulo_1")       # 22,500 frames (remaining)
```

**Storage calculation**:

- Modulo 16: ~1,875 frames
- Modulo 4: ~5,625 frames
- Modulo 1: ~22,500 frames
- **Total: ~30,000 frames (NO duplication, 1.0x)**
- With VP9 compression: **~30MB per video**

**Benefits of non-duplicating approach**:

- ✅ **No storage waste**: Every frame stored once (vs 1.25x with duplication)
- ✅ **60% storage reduction** (~30MB vs ~75MB)
- ✅ **Lower bandwidth**: No duplicate frames to download
- ✅ **Simpler cache management**: No deduplication logic needed
- ✅ **Better coverage**: Same progressive loading, less storage

### Three-Tier Loading Strategy

**1. Jump Loading (HIGHEST PRIORITY)**:

When user explicitly navigates (Jump to Frame, Prev button):

- **Preloading pauses immediately** (yields bandwidth)
- Loads modulo_1 (finest) frames FIRST around jump target (range ±32)
- **Blocks navigation** until exact frames loaded
- Ensures user sees high-quality frames immediately on arrival
- Then continues with normal progressive loading

**2. Normal Progressive Loading (MEDIUM PRIORITY)**:

During normal scrolling/annotation work:

- Loads coarse-to-fine: modulo_16 → modulo_4 → modulo_1
- Centered on current frame position
- Ranges: 16=±512, 4=±128, 1=±32 frames
- Triggers when moved >3 frames
- Runs continuously (100ms polling)

**3. Next Annotation Preloading (LOWEST PRIORITY)**:

Background optimization for seamless "Next" workflow:

- Starts **immediately** when next annotation identified
- Runs once per annotation (tracked by ID)
- **Automatically yields** to explicit jumps
- For short annotations (<500 frames):
  - Loads ALL modulos completely
  - User sees instant high-quality frames when clicking "Next"
- For long annotations (>500 frames):
  - Loads modulo_16 across annotation (overview)
  - Loads modulo_4 near boundaries (precision)
  - Defers modulo_1 until user arrives

```typescript
// Three-tier priority system
function loadFrames() {
  // Priority 1: Explicit jump (blocks navigation)
  if (jumpRequested) {
    await loadJumpFrames(jumpTarget, (modulo = 1), (range = 32))
    completeNavigation()
  }

  // Priority 2: Normal progressive (continuous)
  await loadProgressiveFrames(currentFrame, [16, 4, 1])

  // Priority 3: Next annotation preload (yields to jumps)
  if (!jumpRequested && nextAnnotation) {
    await preloadNextAnnotation(nextAnnotation)
  }
}
```

### WebM VP9 Format

**Chunk size**: 32 frames per chunk

**Format**: WebM container with VP9 video codec

- 32 frames encoded as short video segment
- Exploits temporal similarity between subtitle frames
- Hardware-accelerated decode (GPU)

**Encoding parameters**:

```bash
ffmpeg -i frames_%04d.jpg \
  -c:v libvpx-vp9 \
  -crf 30 \
  -b:v 0 \
  -row-mt 1 \
  -g 32 \
  chunk.webm
```

### Browser Frame Cache with Smart Pinning

**Cache Strategy**: Protect current and next annotation frames from eviction

- Per-modulo cache limits: modulo_16=40 chunks, modulo_4=50 chunks, modulo_1=60 chunks
- Total cache: ~75-130MB (very reasonable for modern browsers)

**Cache Pinning**:

- Active annotation: Frames from `[start-20, end+20]` are PINNED (never evicted)
- Next annotation: Frames from `[start-20, end+20]` are PINNED (never evicted)
- Other chunks: LRU eviction when over per-modulo limits

```typescript
function isChunkPinned(
  chunkStart: number,
  modulo: number,
  activeAnnotation: Annotation | null,
  nextAnnotation: Annotation | null
): boolean {
  const BOUNDARY_BUFFER = 20 // Frames around annotation boundaries
  const chunkEnd = chunkStart + 32 * modulo - 1

  // Check active annotation (with ±20 buffer)
  if (activeAnnotation) {
    const start = Math.max(0, activeAnnotation.start - BOUNDARY_BUFFER)
    const end = activeAnnotation.end + BOUNDARY_BUFFER
    if (chunkStart <= end && chunkEnd >= start) return true
  }

  // Check next annotation (with ±20 buffer)
  if (nextAnnotation) {
    const start = Math.max(0, nextAnnotation.start - BOUNDARY_BUFFER)
    const end = nextAnnotation.end + BOUNDARY_BUFFER
    if (chunkStart <= end && chunkEnd >= start) return true
  }

  return false
}
```

**Benefits**:

- ✅ **Instant navigation to next annotation** - frames already loaded & pinned
- ✅ **Smooth boundary adjustments** - ±20 buffer ensures frames available
- ✅ **No thrashing** - pinned chunks protected during navigation
- ✅ **Large cache** - 150 chunks total supports multiple annotations resident

### Frame Extraction

```javascript
// Load chunk
const video = document.createElement('video')
video.src = chunkUrl
await video.play()

// Extract frames sequentially
for (let i = 0; i < 32; i++) {
  const frameIndex = chunkStartIndex + i

  // Skip if already cached from higher modulo
  if (frameCache.has(frameIndex)) continue

  video.currentTime = i / 32.0 // Seek to frame position
  await new Promise(r => video.addEventListener('seeked', r, { once: true }))

  // Draw to canvas
  const canvas = new OffscreenCanvas(video.videoWidth, video.videoHeight)
  canvas.getContext('2d').drawImage(video, 0, 0)
  const bitmap = canvas.transferToImageBitmap()

  frameCache.set(frameIndex, bitmap)
}
```

### Performance Analysis

**Storage per video**: ~70-80MB (vs 90MB pure no-dup, 360MB full dup)

**Typical load scenario** (short annotation, ±50 frames):

- Modulo 4 chunks: 2-3 chunks × ~15KB = ~45KB
- Modulo 1 chunks: 3-4 chunks × ~20KB = ~80KB
- Total download: ~125KB
- Decode time: ~100-150ms (hardware-accelerated)
- **Total time: ~200-300ms**

**Jump scenario** (long scroll, ±200 frames):

- Modulo 16: 2 chunks (~30KB) - coarse view with good coverage (~25 frames)
- Modulo 4: 7 chunks (~105KB) - smooth detail
- Modulo 1: 13 chunks (~260KB) - complete
- **Progressive refinement: 50ms → 150ms → 300ms**

### Why WebM VP9

- ✅ **Better compression**: 20-30% smaller than H.264
- ✅ **Universal browser support** (2026): Safari 14.1+ (April 2021), Chrome, Firefox, Edge
- ✅ **Hardware decode**: GPU-accelerated on modern devices
- ✅ **Open format**: No licensing concerns
- ✅ **Sequential decode optimized**: Perfect for our chunk-based architecture

---

## Access Patterns

### Current Flow (SQLite)

```
Browser requests: GET /api/frames/{videoId}/batch?indices=0,32,64,...
  ↓
Backend queries SQLite for 32 frames (cropped_frames table)
  ↓
Backend base64-encodes each frame
  ↓
Backend returns JSON: { frames: [{ frame_index, image_data: "base64..." }] }
  ↓
Browser decodes base64, creates object URLs
  ↓
Browser displays images
```

**Bandwidth**: 2x fly.io egress (SQLite → backend → browser)

### New Flow (Wasabi)

```
Browser requests: GET /api/frames/{videoId}/batch-signed-urls?indices=0,32,64,...
  ↓
Backend authorizes user (Supabase session)
  ↓
Backend verifies user owns videoId
  ↓
Backend generates 32 signed URLs (1 hour expiry, GetObject permission)
  ↓
Backend returns JSON: { frames: [{ frame_index, signed_url }] }
  ↓
Browser fetches 32 JPEGs from Wasabi in parallel (HTTP/2 multiplexing)
  ↓
Browser creates object URLs and displays images
```

**Bandwidth**: 1x fly.io egress (signed URLs only, ~5KB) + free Wasabi egress

### Modulo-Based Hierarchical Loading

**Current Implementation** (`useBoundaryFrameLoader.ts`):

**Three-tier priority system**:

1. **Jump Loading** (highest priority):
   - Explicit navigation (Jump to Frame, Prev button)
   - Preloading pauses immediately
   - Loads modulo_1 (finest) frames FIRST at jump target
   - Blocks navigation until frames loaded
   - Then continues normal progressive loading

2. **Normal Progressive Loading** (medium priority):
   - Loads coarse-to-fine: modulo_16 → modulo_4 → modulo_1
   - Ranges: [16: ±512, 4: ±128, 1: ±32 frames]
   - Triggers when moved >3 frames
   - Runs continuously (100ms polling)

3. **Next Annotation Preloading** (lowest priority):
   - Starts immediately when next annotation identified
   - Yields to explicit jumps
   - Loads ALL modulos for short annotations (<500 frames)
   - Loads overview + boundaries for long annotations

**Cache Management**:

- Per-modulo limits: 40/50/60 chunks (modulo 16/4/1)
- Smart pinning: Active + next annotation ±20 frames protected
- LRU eviction for unpinned chunks
- Concurrent fetches: up to 8 chunks in parallel

**Integration with Wasabi**:

1. Frontend calculates which frames to fetch (three-tier priority)
2. Frontend requests batch of signed URLs from API
3. Frontend fetches VP9 WebM chunks from Wasabi
4. Frontend decodes, caches, and displays frames

---

## Implementation Plan

### Prerequisites

- No existing users → No migration needed
- No need for SQLite fallback → Clean implementation
- Supabase auth not ready → Use placeholder `default_user` initially

### Week 1-2: Foundation

**Goals**:

- Set up Wasabi infrastructure
- Implement core storage service
- Update frame extraction to write to Wasabi

**Tasks**:

1. **Set up Wasabi account**
   - Create bucket: `captionacc-media`
   - Generate API keys (backend-only, read/write)
   - Configure IAM policy (least privilege)
   - Set environment variables

2. **Implement storage service** (NEW FILE)
   - File: `apps/captionacc-web/app/services/wasabi-storage.server.ts`
   - S3 client initialization
   - `uploadFrame(userId, videoId, frameType, frameIndex, imageData)` - Upload frame to Wasabi
   - `generateSignedUrl(key, expiresIn)` - Generate download-only signed URL
   - `buildFrameKey(userId, videoId, frameType, frameIndex)` - Build S3 key from components
   - Path templates for all content types

3. **Update frame extraction pipeline** (MODIFY)
   - File: `packages/frames_db/src/frames_db/storage.py`
   - Add `write_frame_to_wasabi()` function using boto3
   - **Remove SQLite BLOB storage** (replace, not dual-write)
   - Update `write_frames_batch()` to upload to Wasabi instead of SQLite
   - Keep frame metadata in SQLite (frame_index, width, height) for now

4. **Update database schema** (MODIFY)
   - File: `apps/captionacc-web/app/db/annotations-schema-v2.sql`
   - Modify `full_frames` table: Remove `image_data BLOB`, add `storage_location TEXT`
   - Modify `cropped_frames` table: Remove `image_data BLOB`, add `storage_location TEXT`
   - `storage_location` stores relative Wasabi key: `full_frames/frame_0000000100.jpg`

5. **Environment configuration**
   ```bash
   # .env
   WASABI_ACCESS_KEY=...
   WASABI_SECRET_KEY=...
   WASABI_BUCKET=captionacc-media
   WASABI_REGION=us-east-1
   WASABI_ENDPOINT=https://s3.us-east-1.wasabisys.com
   ENVIRONMENT=dev  # or prod
   ```

**Deliverables**:

- ✅ Wasabi bucket configured with proper IAM
- ✅ Storage service implemented and tested
- ✅ Frame extraction writes to Wasabi (no SQLite BLOBs)
- ✅ Database tracks frame locations (not frame data)

### Week 3-4: Browser Access

**Goals**:

- Implement batch signed URLs endpoint
- Update frontend to fetch from Wasabi
- Remove SQLite BLOB queries

**Tasks**:

1. **Implement batch signed URLs endpoint** (NEW FILE)
   - File: `apps/captionacc-web/app/routes/api.frames.$videoId.batch-signed-urls.tsx`
   - Parse comma-separated frame indices from query string
   - Query database for `storage_location` for each frame
   - Generate signed URLs for each frame (GetObject, 1 hour expiry)
   - Return JSON: `[{ frame_index, signed_url, width, height }]`
   - Cache response (1 hour)

2. **Update frontend frame loader** (MODIFY)
   - File: `apps/captionacc-web/app/hooks/useBoundaryFrameLoader.ts`
   - Change endpoint: `/batch` → `/batch-signed-urls`
   - Parse response: Extract `signed_url` instead of `image_data`
   - Fetch from Wasabi: `fetch(signed_url)` instead of `atob(image_data)`
   - Rest of logic unchanged (caching, modulo groups, LRU, etc.)

3. **Remove old batch endpoint** (DELETE or DEPRECATE)
   - File: `apps/captionacc-web/app/routes/api.frames.$videoId.batch.tsx`
   - No longer needed (was querying SQLite BLOBs)
   - Keep temporarily for backward compatibility, then remove

4. **Update single frame endpoint** (MODIFY)
   - File: `apps/captionacc-web/app/routes/api.frames.$videoId.$frameIndex[.jpg].tsx`
   - Query database for frame's `storage_location`
   - Generate signed URL
   - Return 302 redirect to signed URL (or return signed URL as JSON)

**Deliverables**:

- ✅ Batch signed URLs endpoint working
- ✅ Frontend fetches frames from Wasabi
- ✅ Boundaries annotation workflow functional
- ✅ No more SQLite BLOB queries

### Week 5-6: Supabase Auth Integration

**Goals**:

- Integrate Supabase authentication
- Add authorization checks
- Replace `default_user` with real user IDs

**Tasks**:

1. **Set up Supabase client** (MODIFY)
   - Add Supabase SDK to dependencies
   - Initialize Supabase client with environment variables
   - Implement session extraction from request cookies

2. **Add authorization middleware** (NEW FILE)
   - File: `apps/captionacc-web/app/middleware/auth.server.ts`
   - `requireAuth(request)` - Extract and verify Supabase session
   - `requireVideoOwnership(userId, videoId)` - Check user owns video
   - Returns 401 if not authenticated, 403 if not authorized

3. **Update video metadata schema** (MODIFY)
   - File: `apps/captionacc-web/app/db/annotations-schema-v2.sql`
   - Add `owner_user_id TEXT` column to `video_metadata` table
   - Populate with `default_user` for existing videos
   - Update all video creation to include owner_user_id

4. **Update batch signed URLs endpoint** (MODIFY)
   - File: `apps/captionacc-web/app/routes/api.frames.$videoId.batch-signed-urls.tsx`
   - Add authorization: `await requireAuth(request)`
   - Check ownership: `await requireVideoOwnership(session.user.id, videoId)`
   - Use real userId in Wasabi paths (replace `default_user`)

5. **Add rate limiting** (NEW FILE)
   - File: `apps/captionacc-web/app/services/rate-limiter.server.ts`
   - Per-user rate limiting: 1000 requests per minute
   - Return 429 if exceeded

6. **Update frame extraction** (MODIFY)
   - File: `packages/frames_db/src/frames_db/storage.py`
   - Accept `user_id` parameter (from video metadata)
   - Use real userId in Wasabi paths instead of `default_user`

**Deliverables**:

- ✅ Supabase auth integrated
- ✅ Authorization checks in all endpoints
- ✅ Real user IDs used in Wasabi paths
- ✅ Rate limiting active
- ✅ Secure multi-tenant architecture

---

## Critical Files

### New Files (Create)

1. **`apps/captionacc-web/app/services/wasabi-storage.server.ts`**
   - S3 client initialization
   - Upload functions
   - Signed URL generation
   - Path building utilities

2. **`apps/captionacc-web/app/routes/api.frames.$videoId.batch-signed-urls.tsx`**
   - Batch signed URL endpoint
   - Authorization
   - Signed URL array response

3. **`apps/captionacc-web/app/middleware/auth.server.ts`**
   - Supabase session extraction
   - Authorization helpers

4. **`apps/captionacc-web/app/services/rate-limiter.server.ts`**
   - Per-user rate limiting

### Files to Modify

1. **`apps/captionacc-web/app/routes/api.frames.$videoId.$frameIndex[.jpg].tsx`**
   - Remove SQLite BLOB query
   - Generate signed URL, return redirect

2. **`apps/captionacc-web/app/hooks/useBoundaryFrameLoader.ts`**
   - Change endpoint to `/batch-signed-urls`
   - Fetch from Wasabi instead of decoding base64

3. **`packages/frames_db/src/frames_db/storage.py`**
   - Add `write_frame_to_wasabi()` function
   - Remove SQLite BLOB storage
   - Add `user_id` parameter

4. **`apps/captionacc-web/app/db/annotations-schema-v2.sql`**
   - Remove `image_data BLOB` columns
   - Add `storage_location TEXT` columns
   - Add `owner_user_id TEXT` to video_metadata

### Files to Delete

1. **`apps/captionacc-web/app/routes/api.frames.$videoId.batch.tsx`**
   - Old endpoint that queries SQLite BLOBs
   - Replaced by batch-signed-urls endpoint

---

## Lifecycle Management

### User Deletion

```typescript
export async function deleteUserData(userId: string): Promise<void> {
  const env = process.env.ENVIRONMENT ?? 'dev'
  const prefix = `${env}/users/${userId}/`

  // List all objects for user (videos, frames, databases, everything)
  const objects = await listAllObjects(process.env.WASABI_BUCKET!, prefix)

  // Delete in batches of 1000 (S3 limit)
  for (let i = 0; i < objects.length; i += 1000) {
    const batch = objects.slice(i, i + 1000)
    await s3Client.send(
      new DeleteObjectsCommand({
        Bucket: process.env.WASABI_BUCKET!,
        Delete: {
          Objects: batch.map(key => ({ Key: key })),
        },
      })
    )
  }
}
```

**Result**: All user data deleted with single prefix deletion (videos, frames, databases, etc.)

### Video Deletion

```typescript
export async function deleteVideoData(userId: string, videoId: string): Promise<void> {
  const env = process.env.ENVIRONMENT ?? 'dev'
  const prefix = `${env}/users/${userId}/videos/${videoId}/`

  // List all objects for video (all content types)
  const objects = await listAllObjects(process.env.WASABI_BUCKET!, prefix)

  // Delete all objects
  await s3Client.send(
    new DeleteObjectsCommand({
      Bucket: process.env.WASABI_BUCKET!,
      Delete: {
        Objects: objects.map(key => ({ Key: key })),
      },
    })
  )
}
```

**Result**: All video data deleted (original video, all frame types, databases, etc.)

### Soft Delete (Optional)

For 30-day retention before permanent deletion:

```typescript
// Tag objects for deletion instead of immediate delete
await s3Client.send(
  new PutObjectTaggingCommand({
    Bucket: bucket,
    Key: `${env}/users/${userId}/videos/${videoId}/`,
    Tagging: {
      TagSet: [
        { Key: 'deleted', Value: 'true' },
        { Key: 'deleted_at', Value: new Date().toISOString() },
      ],
    },
  })
)
```

Configure Wasabi lifecycle rule in console:

- Delete objects tagged `deleted=true` after 30 days
- Automatic cleanup, no cron jobs needed

---

## Cost Analysis

### Current (SQLite on fly.io)

**Assumptions**: 100 users, 1,000 videos

| Item                    | Calculation                                                | Cost          |
| ----------------------- | ---------------------------------------------------------- | ------------- |
| Storage (fly.io volume) | 205MB/video × 1,000 videos = 205GB × $0.15/GB/mo           | $30.75/mo     |
| Bandwidth (egress)      | 100 users × 125KB/load × 20 loads/day × 30 days × $0.02/GB | $15.00/mo     |
| **Total**               |                                                            | **$45.75/mo** |

### Projected (Wasabi with VP9 Chunks)

**Same assumptions, hybrid duplication (1.25x)**

| Item               | Calculation                                                       | Cost         |
| ------------------ | ----------------------------------------------------------------- | ------------ |
| Storage            | 75MB/video × 1,000 videos = 75GB × $0.0059/GB/mo                  | $0.44/mo     |
| Bandwidth (egress) | FREE                                                              | $0.00/mo     |
| API requests       | 100 users × 6 chunks/load × 20 loads/day × 30 days × $0.0004/1000 | $0.14/mo     |
| **Total**          |                                                                   | **$0.58/mo** |

### Savings

- **Monthly**: $45.17 (99% reduction)
- **Annual**: $542
- **At 1,000 users**: $5,420/year savings

**Storage per video**: 75MB (vs 205MB SQLite BLOBs)

- 64% reduction in storage size
- 1.25x duplication ratio (hybrid approach)
- Free egress bandwidth

**Break-even**: Immediate (Wasabi cheaper from day 1)

---

## Risk Mitigation

### Risk: Wasabi outage

**Impact**: Users cannot view frames

**Mitigation**:

- Monitor Wasabi status page
- Set up alerts for API errors
- Consider multi-region replication for production (future)

### Risk: Cost overrun

**Impact**: Unexpected storage/API costs

**Mitigation**:

- Set up billing alerts in Wasabi console (alert at 80% of budget)
- Monitor storage usage per user in application
- Implement per-user quotas based on subscription tier
- Regular cost audits

### Risk: Security breach (leaked signed URLs)

**Impact**: Unauthorized access to frames

**Mitigation**:

- Short expiry (1 hour)
- Download-only permissions (GetObject, not PutObject)
- Rate limiting per user
- Monitor for unusual access patterns (alert on 10x normal usage)

### Risk: Performance degradation

**Impact**: Slower frame loading

**Mitigation**:

- Monitor p95, p99 latency
- Compare against SQLite baseline (<200ms per batch)
- Aggressive browser caching (Cache-Control: immutable)
- HTTP/2 multiplexing for parallel fetches
- Alert if latency exceeds 500ms

---

## Success Metrics

| Milestone | Metric                            | Target                      |
| --------- | --------------------------------- | --------------------------- |
| Week 2    | New videos write to Wasabi        | 100% success rate           |
| Week 2    | Wasabi upload errors              | 0 errors                    |
| Week 4    | Frame requests served from Wasabi | 100%                        |
| Week 4    | Bandwidth cost reduction          | >90% vs baseline            |
| Week 4    | Frame loading latency (p95)       | <500ms                      |
| Week 6    | Supabase auth integrated          | 100% requests authorized    |
| Week 6    | Database size reduction           | <25MB (vs 205MB baseline)   |
| Week 6    | Monthly cost                      | <$2/mo (vs $32/mo baseline) |

---

## Alternative Approaches Considered

### Alternative 1: Supabase Storage

**Pros**: Integrated with Supabase Auth (simpler RLS)

**Cons**:

- **Expensive**: $0.021/GB storage + $0.09/GB egress
- At 193GB: $4/mo storage + $52/mo bandwidth = **$56/mo** (vs $1.20/mo Wasabi)

**Verdict**: Only if tight Supabase integration worth 47x cost

### Alternative 2: Cloudflare R2

**Pros**: Free egress (like Wasabi), Cloudflare CDN integration

**Cons**:

- **More expensive storage**: $0.015/GB vs $0.0059/GB Wasabi
- At 193GB: $2.90/mo (2.4x more than Wasabi)
- Requires Cloudflare account (additional service to manage)

**Verdict**: Good alternative if already using Cloudflare

### Alternative 3: Store Chunk Files Instead of Individual Frames

**Already discussed above** - see "Data Storage Design" section.

**Verdict**: Individual frames recommended (flexibility, no duplication, matches current design)

### Alternative 4: Keep SQLite BLOBs

**Pros**: Simple, no external service

**Cons**:

- **Expensive bandwidth**: 2x fly.io egress
- **Large backups**: 205MB database per video
- **Slow queries**: BLOB extraction slower than HTTP fetch

**Verdict**: Only viable for <50 users

---

## FAQ

### Why not use CDN?

**Answer**: CDN is for serving the same content to many users. In our app, each user only views their own frames. No content sharing = no CDN benefit.

### Why individual frames instead of chunk files?

**Answer**:

- Flexibility: Modulo groups are dynamic (user position changes)
- No duplication: 180MB vs 1GB+ with chunks
- Simpler: Matches current design
- Cost difference negligible: $0.04/mo

### Why download-only for users?

**Answer**: All videos must be processed before storage (frame extraction, OCR, etc.). Backend mediates all uploads to ensure quality and security.

### Why not dual-write (SQLite + Wasabi)?

**Answer**: No existing users = no migration risk. Clean implementation is simpler and avoids temporary complexity.

### Do we need SQLite fallback?

**Answer**: No. Since there are no users yet, we can do a clean migration. If Wasabi has issues, we can restore from backups and redeploy with SQLite temporarily.

### What about Wasabi regional availability?

**Answer**: Start with `us-east-1`. If you have global users, consider multi-region replication later (Wasabi supports this).

---

## Next Steps

1. ✅ Review this architecture plan
2. ⏳ Clarify any remaining questions
3. ⏳ Get approval to proceed
4. ⏳ Set up Wasabi account and bucket
5. ⏳ Begin Week 1 implementation

---

**Questions or concerns?** Please review and provide feedback before we proceed with implementation.
